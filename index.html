---
layout: page
title: Chenghao Yang
subtitle: Welcome to my personal website!
---

<div>
    <!-- <blockquote>
    <p>
        <mark>[Attention!]</mark> We are organizing the First Workshop on Theory of Mind in Communicating Agents <a href="https://tomworkshop.github.io/">[ToM 2023 website]</a>! 
        The submission phase has now concluded and we sincerely thanks for all great submissions and great efforts from the authors, reviewers and area chairs. Looking forward to meet with you at Hawaii this summer!
    --ignore below part---
        If you are interested in how we can build model for other agents's belief and thought, we highly encourage you to submit a paper to our workshop and join our workshop this summer at Hawaii!
        We encourage under-review, accepted and new submissions (thought pieces, position papers and empirical papers) with 2 to 8 pages length, and all accepted papers are non-archival. So it is relatively easy to submit and share your great work! 
    </p>
    </blockquote> -->

    <p>My name is Chenghao Yang. I am currently a (x-2022)-year Ph.D. student at University of Chicago. I am fortunately advised by
        <a href="https://aetting.github.io/">Prof. Allyson Ettinger</a> (on leave). 
        My research is generously supported by <a href="https://physicalsciences.uchicago.edu/academics/financial-aid/eckhardt-scholars/">Eckhardt Scholarship</a>. 
        I also work closely with <a href="https://www.haifeng-xu.com/">Prof. Haifeng Xu</a> and <a href="https://ariholtzman.com/">Prof. Ari Holtzman</a> at UChicago.
    </p>
    <p>[Highlight]: <strong>I am loooking for full-time opportunities in industry!</strong> Would love to talk if you find us a good match.</p>
    <p>My research interest focuses on natural language processing (NLP) and Machine Learning (ML). 
       I strongly believe AI agents would be or already are super-intelligent and my goal is to facilitate human-AI communication.
       This essentially boils down to two research questions: 1) How can we help humans understand machine communication? 2) How can we help machine to understand human communication?
      Specifically, I worked on pragmatics (Check out our <a href="https://tomworkshop.github.io/">ToM 2023 workshop</a>), Strategic Human-LLM Communication, <a href="https://github.com/thunlp/TAADpapers">adversarial robustness</a>, and <a href="https://github.com/yangalan123/TemporalPointProcessPapers">continuous-time event stream modeling</a>. </p>
    <p>
        <!-- I also worked as RA at Johns Hopkins University  -->
        <!-- (advised by <a href="https://www.cs.jhu.edu/~jason/">Prof. Jason Eisner</a>).      -->
        <!-- I am fortunately . -->
        <!-- I keep close connections and collaboration with my previous advisors and mentors (Great thanks!):  -->
        I am fortunate to be guided by so many great reseachers and I deeply appreciate their help (Not an extensive list): 
        <!-- with  at Johns
        Hopkins University, -->
        <a href="https://www.cs.jhu.edu/~jason/">Prof. Jason Eisner</a> at JHU,
        <a href="https://hhexiy.github.io/">Prof. He He</a> at NYU, 
        <a href="http://web.cs.ucla.edu/~kwchang/">Prof. Kai-Wei Chang</a> at UCLA,
        <a href="https://xuezhemax.github.io/">Prof. Xuezhe Ma</a> at USC, 
        <a href="http://www.cs.columbia.edu/~smara/">Prof. Smaranda Muresan</a> at Columbia University, 
        <a href="http://nlp.csai.tsinghua.edu.cn/~lzy/">Prof. Zhiyuan Liu</a> at Tsinghua University.</p>
        <p>Also shout out to my great mentors:
        <a href="https://scholar.google.com/citations?user=ZP4gfYcAAAAJ&hl=en">Dr. Zi Yang</a> (when I worked at Google Research),
        <a href="https://yuhao.im/">Dr. Yuhao Zhang</a> (when I worked at Amazon AI Labs),
        <a href="https://www.hongyuanmei.com/">Dr. Hongyuan Mei</a> (when I worked at JHU),
        <a href="https://sites.google.com/site/moyunlp/">Dr. Mo Yu</a> (when I worked at IBM Watson).
        </p>

    <p>Before I join UChicago, I was previously an applied scientist at AWS AI, 
        under the lead of <a href="https://www.andrewoarnold.com/">Andrew O. Arnold. </a>
        My full-time work at AWS AI is mostly about building and evaluating large-scale language models for code generation (Check out our great <a href="https://aws.amazon.com/codewhisperer/">AWS Codewhisper</a>!). 
        I obtained my M.S. degree in Computer Science Department at Columbia University (ML track) 
        and my bachelor's degree in Software College, Beihang University.</p>

    
    
    <p>I enjoy listening to music, playing the guitar, watching movies, and the anime in my personal spare time.
        I recently become fascinated by cooking Chinese dishes. </p>

    <p>I believe the famous quote <a href="https://en.wikipedia.org/wiki/Frank_Oppenheimer#:~:text=Oppenheimer%20also%20worked%20with%20the,to%20learn%20is%20to%20teach%22.">
        "The best way to learn is to teach."</a> I often make many <a href="https://yangalan123.github.io/xlog">slides</a> or do some chalk talks to explain novel concepts or technical ideas. 
        <!-- See the collection of my slides here at <a href="https://drive.google.com/drive/folders/1YOjk_gGUptvVaSuafamo71r_RHEmsPm2?usp=share_link">Google Drive</a>  -->
        Recently: 
        <a href="https://docs.google.com/presentation/d/16ZSH6uOKDY7jILUyRxAH3-gywox_KUmP/edit?usp=drive_link&ouid=111912319459945992784&rtpof=true&sd=true">Tutorial for Inference-Time Compute</a>,
        <a href="https://chenghao-yang.notion.site/By-Alignment-What-are-we-aligning-LLMs-to-75be789fe7884a03816c4314a0681c2c?pvs=74">Thoughts and Learning Notes for Human-LLM Alignment</a>,
        <a href="https://docs.google.com/presentation/d/1oIPHP_7qjsrnrDb3kdZIUZt-wQofkiQl/edit?usp=sharing&ouid=111912319459945992784&rtpof=true&sd=true">Tutorial for Mechanistic Interpretability (RASP)</a>,
        <a href="https://docs.google.com/presentation/d/1AduB4KWxX_vLuPmHGsy5r2NiGz8o1w7G/edit?usp=sharing&ouid=111912319459945992784&rtpof=true&sd=true">Distributed Training and Inference in Academia Environment</a>,
        <a href="https://docs.google.com/presentation/d/1Qlb3zTLSSpsyk0r27Qn2zBNl_40wO-sZ/edit?usp=sharing&ouid=111912319459945992784&rtpof=true&sd=true">External Tools + LLM (WebGPT, Toolformer)</a>,
        <a href="https://docs.google.com/presentation/d/1QDWR6DgHpeYFeOPBG-V0uy9KRzVaHyif/edit?usp=sharing&ouid=111912319459945992784&rtpof=true&sd=true">GPT-4 TechReport 100-Page Walkthrough</a>,
        <a href="https://docs.google.com/presentation/d/1eHWRBEmAYVW_y5PMavhmuY6pS0LtvSFY/edit?usp=share_link&ouid=111912319459945992784&rtpof=true&sd=true">Emergent Ability of LLM</a>
        <a href="https://docs.google.com/presentation/d/1ujdbaLNRAN7YcdUsYisR837Op0Tu9e27/edit?usp=share_link&ouid=111912319459945992784&rtpof=true&sd=true">Unified View of Pattern Efficient Tuning</a>,
        <!-- <a href="https://drive.google.com/file/d/1RpGkUbsBSgIeVLy6IOqq2e3XL39Zoon-/view?usp=share_link">Open-Retrieval QA Pretraining: Explained</a>,
        <a href="https://drive.google.com/file/d/1Er7gXmnsNmzyA95mxWaPpjuGe5VEpbFv/view?usp=share_link">Tutorial on Certified Robustness in NLP</a> -->
        <!-- ) -->
    </p>
        Feel free to send me any comments or feedbacks! I am happy to chat about various topics in ML and NLP. 
        I am also open to various form of research collaboration. 
    
    
</div>

<div class='text-center'>
    <hr>
    <p>
       [
       <a
           href="https://drive.google.com/file/d/1fJKtmvnz_kdwXeO_NBd-W2X1W1TKMrVv/view?usp=sharing">
           CV (updated: May. 2025)
        </a>
       ]

       [
       <a href="https://scholar.google.com/citations?user=B28fiOAAAAAJ&hl=zh-CN">
       Google Scholar
       </a>
       ]

       [
       <a href="https://www.linkedin.com/in/chenghao-yang-857b51178/">
       LinkedIn
    </a>
       ]
        [
        <a href="https://twitter.com/chrome1996">
            Twitter
        </a>
        ]
       [
       <a href="https://github.com/yangalan123">
       Github
    </a>
       ]

       <!-- [
       <a href="#misc">
        PhD Applicati
       </a>
       ] -->
   </p>
    <hr>
</div>
<div>
    <h2>
        Personal News
    </h2>
</div>
<div>
    <ul style="height: 300px; overflow: auto">
        <li>[May, 2025] Due to logistics pressure, I have to graduate soon and I am looking for a full-time job in industry! I would love to talk if you find us a good match!</li>
        <li>[April-May, 2025] Excited to be invited to give a talk on AI-Realtor at Salesforce AI Research Future Forum and <a href="https://nlp.nd.edu/msld25/">MSLD 2025!</a></li>
        <li>[Feb, 2025] Thrilled to introduce <a href="https://yangalan123.github.io/ai-realtor/">AI-Realtor</a>, our Economic-Grounded Personalized Persuasion Agent work! Huge thanks to all of my interdiscplinary collaborators!</li>
        <li>[Nov, 2024] Honored to receive the University Fellowship and be nominated for Two-Sigma Ph.D. Fellowship!</li>
        <li>[April, 2024] Excited to share my two recent papers get accepted to NAACL 2024 Findings! Kudos to all my collaborators! One is about my four-year efforts collaborating with social scientist and medical experts on Opiate Addiction Identification via social media self-disclosure. The other one is led by my amazing mentee Yanhong Li on evaluating reflective thinking for LLMs!</li>
        <li>[Jan, 2024] Excited to share my first collaboration work with Chaoqi, Yibo, Han and Yuxin on DPO has been accepted to ICLR 2024 as a SPOTLIGHT! We propose f-DPO, allowing flexible trade-off for generation diversity, calibration and alignment. Read more <a href="https://arxiv.org/abs/2309.16240">[here]</a></li>
        <li>[Oct, 2023] Excited to share that my paper with Allyson has been accepted at EMNLP 2023! In this paper, we propose a new synthetic environment to test the situational understanding for ChatGPT, and we find ChatGPT has non-persistent in-context memory. Read more <a href="https://arxiv.org/abs/2310.16135">[here]</a></li>
        <li>[July, 2023] Successfully organize our workshop ToM 2023 @ ICML 2023! 
            Thanks all great efforts from our speakers, panelists, authors, reviewers, co-organizers and advisory board members! See you next workshop!</li>
        <li>[June, 2023] Start my internship at Google as a student researcher! I will work on building new large language models. </li>
        <li>
            [May, 2023] Excited to share my two works mainly done at AWS has been officially accepted! 
            Big thanks for all my collaborators! 
            <ul>
                <li>
                    One (<a href="https://twitter.com/chrome1996/status/1678105369388646401">Amortized-Interpretation</a>) is on Shapley Values explanation with my great advisor Prof. He He and Prof. Kai-Wei Chang. 
                    We identify new stability-efficiency trade-off issues with commonly used Shapley Values settings. We develop a fast amortized model that can achieve 60-600 times efficiency boosting while maintaining good faithfulness and performance on downstream tasks. 
                </li>
                <li>
                    The other paper (<a href="https://www.linkedin.com/posts/chenghao-yang-857b51178_amazonscience-codegeneration-codewhisperer-activity-7016289847258615808-zjHN?utm_source=share&utm_medium=member_desktop">ReCode</a>) is a comprehensive robustness benchmark on code generation models (the first robustness benchmark on generation models!) that I first time mentor/co-mentor non-NLP PhD students to do evaluation on LLMs. 
                </li>
            </ul>
        </li>
        <li>
            [Apr, 2023] We are organizing the First Workshop on Theory of Mind in Communicating Agents <a href="https://tomworkshop.github.io/">[website]</a>! 
            If you are interested in how we can build model for other agents's belief and thought, we highly encourage you to submit a paper to our workshop and join our workshop this summer at Hawaii!
            We encourage under-review, accepted and new submissions (thought pieces, position papers and empirical papers) with 2 to 8 pages length, and all accepted papers are non-archival. So it is relatively easy to submit and share your great work!
        </li>
        <li>
            [Nov, 2022] Gives a Talk on Predicting and Explaining Message-Level Disclosures of Opioid Use Disorder at University of Maryland, College Park. 
        </li>
        <li>
            [Oct, 2022] Excited to share that my work collaborated with Prof. Xuezhe (Max) Ma at USC has been accepted to EMNLP 2022! 
            Shout out to my great mentor Max and thanks for help from Marius Mosbach!
        </li>
        <li>
            [Sept, 2022] Officially start my Ph.D. at UChicago!
        </li>
        <li>
            [Aug, 2022] Last days in AWS AI. Thanks for great mentors, colleagues, managers and leaders. I learn a lot. See my official goodbye tweet.
            Looking forward to more exciting news for this excellent team!
        </li>
        <li>
            [July, 2022] Attend NAACL'22 for presenting my TACL'21 paper on NarrativeQA. Say hi if you are in Seattle as well!
        </li>
        <li>
            [June, 2022] Visited USC and UCLA at LA. 
            Thank Robin, Xuezhe, Muhao, Swabha and Kai-Wei for hosting me and having a chat with me in LA!
        </li>
        <li>
            [April, 2022] Officially accept UChicago CS Ph.D. offer. I will work with my great advisors  <a href="https://aetting.github.io/">Prof. Allyson Ettinger</a> and <a href="https://chenhaot.com/">Prof. Chenhao Tan</a>. My research will also be generously supported by UChicago Eckhardt Scholarship.
            Thanks, UChicago! Also excited to host my <a href="https://iclr.cc/virtual/2022/social/8741">first social panel "Better Developing Pretraining-based Models and Beyond"</a> at ICLR 2022!
        </li>
        <li>
            [Jan, 2022] Excited to share that my work collaborated with <a href="https://www.cs.jhu.edu/~jason/">Prof. Jason Eisner</a> and <a href="https://www.hongyuanmei.com/">Prof. Hongyuan Mei</a> 
            has been accepted to ICLR 2022! 
            It mainly focuses on building a neural-symbolic hybrids based on Transformer architecture and can be used for event stream modeling. 
            Please take a look at our <a href="https://arxiv.org/abs/2201.00044">full paper</a> and our <a href="https://github.com/yangalan123/anhp-andtt">codebase</a>!
        </li>
        <li>
            [Oct, 2021] Officially be invited to serve as reviewers for ACL Rolling Reviews. 
            Will work on September (as emergency), October and November. 
        </li>
        <li>
            [June, 2021] Officially join AWS AI as an applied scientist intern. Looking forward to explore Robustness + QA project! 
            Feel free to reach out if you are also at AWS!
        </li>
        <li>
            [May, 2021] Three Important News:
            <ul>
            <li>
                 My TACL paper on NarrativeQA has been officially accepted by TACL (work done during my internship at IBM). 
                 Thanks to my great mentor <a href="https://sites.google.com/site/moyunlp/">Mo Yu</a> and co-authors!
            </li>
            <li>
                My paper on suicide risk assessment has been accepted to ACL 2021 as a short paper! 
                Thanks to my great advisor <a href="http://www.cs.columbia.edu/~smara/">Smara</a> 
                and my supportive co-author <a href="http://www.columbia.edu/~yz3655/index.html">Yudong</a>!
            </li> 
            <li>
                Also, our colaboration works with Columbia School of Social Work on COVID-19 social media analysis 
                has also been officially accepted by Journal of Addiction Medicine Production. Thanks to all great collaborators! 
                Very excited to contribute my efforts on COVID-19 related researches.
            </li> 
            </ul>
        </li>
        <li>
            [April, 2021] Officially graduated with a Master's degree in Computer Science from Columbia. 
            Thanks to my great research advisor <a href="http://www.cs.columbia.edu/~smara/">Smaranda Muresan</a>, 
            my awesome and patient lecturers and TAs and 
            thanks for the accompany of my classmates!
        </li>
        <li>[Oct, 2020] I will start my visiting research assistant at JHU CLSP during Spring 2021.
            I will work with <a href="https://www.cs.jhu.edu/~jason/">Prof. Jason Eisner</a>
            and his PhD advisee <a href="https://www.cs.jhu.edu/~hmei/">Hongyuan Mei</a> on a remote basis.</li>
        <li>
            [Jun, 2020] I start my internship at IBM Watson as a Sr. Cognitive Software Developer.
            I will work with <a href="https://sites.google.com/site/moyunlp/">Dr. Mo Yu</a> on NarrativeQA projects.
            Feel free to connect if you are also at IBM!
        </li>
        <li>[Jan, 2020] I start working as a Research Assistant at Columbia University, working with
            <a href="http://www.cs.columbia.edu/~smara/">Prof. Smaranda Muresan</a> on the topic of NLP for health and
            social good.
        </li>
        <li>
            [Dec, 2019] I finished my visiting at Tsinghua University as a Visiting Student Research Assistant.
            Great thanks for my advisor <a href="http://nlp.csai.tsinghua.edu.cn/~lzy/">Prof. Zhiyuan Liu</a> and my
            great collaborators
            <a href="http://www.zhuhao.me/">Hao Zhu</a>, <a href="http://nlp.csai.tsinghua.edu.cn/~xrb/">Ruobin Xie</a>,
            Fanchao Qi, Yuan Zang and Junjie Huang.
        </li>
    </ul>

</div>
<div>
    <h2 id="publication">Publication</h2>
    <p>(“*” indicates equal contribution)</p>
    <h3 id="ongoing-works">Ongoing Works</h3>
    <ol>
        <li>
            <a href="https://yangalan123.github.io/ai-realtor/">[AI-Realtor]</a> {Jibang Wu*, <strong>Chenghao Yang*</strong>}, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, Haifeng Xu., Grounded Persuasive Language Generation for Automated Marketing. <a href="https://arxiv.org/abs/2502.16810">[Arxiv]</a>
        </li>
    </ol>
    <h3 id="journal-papers">Journal Papers</h3>
    <ol>
        <li>
            {<strong>Chenghao Yang*</strong>, Xiangyang Mou*, Mo Yu*}, Bingsheng Yao, Xiaoxiao Guo, Saloni Potdar, Hui Su., 
            <strong>Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study</strong> TACL 2021 
            <a href="https://arxiv.org/pdf/2106.03826.pdf">
                [paper]
            </a>
        </li>
        <li>Nabila El-Bassel, Karli R Hochstatter, Melissa Slavin, {<strong>Chenghao Yang*</strong>, Yudong Zhang*},
            Smaranda Muresan., <strong>Harnessing the Power of Social Media to Understand the Impact of COVID-19 on People
            Who Use Drugs During Lockdown and Social Distancing</strong>. Journal of Addiction Medicine <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8678390/">[PubMed Paper]</a> 
        </li>
    </ol>
    <h3 id="conference-papers">Conference Papers</h3>
    <ol>
        <li>Suho Shin, <strong>Chenghao Yang</strong>, Haifeng Xu, MohammadTaghi Hajiaghayi., <strong>Tokenized Bandit for LLM Decoding and Alignment</strong>, ICML 2025. 
        <li>{Yanhong Li*, <strong>Chenghao Yang*</strong>}, Allyson Ettinger., <strong>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</strong>, NAACL 2024 Findings <a href="https://arxiv.org/abs/2404.09129">[paper]</a> <a href="https://github.com/yanhong-lbh/LLM-SelfReflection-Eval">[code]</a> </li>
        <li>{<strong>Chenghao Yang*</strong>, Tuhin Chakrabarty*}, Karli R Hochstatter, Melissa N Slavin, Nabila El-Bassel, Smaranda Muresan., <strong>Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts</strong>, NAACL 2024 Findings <a href="https://arxiv.org/abs/2311.09066">[paper]</a> <a href="https://github.com/yangalan123/OpioidID">[code]</a></li>
        <li>Chaoqi Wang, Yibo Jiang, <strong>Chenghao Yang</strong>, Han Liu, Yuxin Chen., <strong>Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints</strong>, ICLR 2024 Spotlight
            <a href="https://arxiv.org/abs/2309.16240">[paper]</a>
    </li>
        <li><strong>Chenghao Yang</strong>, Allyson Ettinger., <strong>Can You Follow Me? Testing Situational Understanding in ChatGPT</strong>, EMNLP 2023
            <a href="https://arxiv.org/abs/2310.16135">[paper]</a><a href="https://github.com/yangalan123/SituationalTesting">[code]</a>
            </li>
        <li><strong>Chenghao Yang</strong>, Fan Yin, He He, Kai-Wei Chang, Xiaofei Ma and Bing Xiang., <strong>Efficient Shapley Values Estimation by Amortization for Text Classification</strong>, ACL 2023 
            <a href="https://arxiv.org/abs/2305.19998">[paper]</a><a href="https://github.com/yangalan123/Amortized-Interpretability">[code]</a><a href="https://www.youtube.com/watch?v=CsCRU8Hzpms">[video]</a>
            </li>
        <li>{Shiqi Wang*, Zheng Li*}, Haifeng Qian, <strong>Chenghao Yang</strong>, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth and Bing Xiang., 
            <strong>ReCode: Robustness Evaluation of Code Generation Models</strong>, ACL 2023 
            <a href="https://arxiv.org/abs/2212.10264">[paper]</a><a href="https://github.com/amazon-science/recode">[codebase]</a>
            <a href="https://dl4c.github.io/assets/pdf/papers/13.pdf">[DL4C @ ICLR 2023 Version]</a>
        </li>
        <li><strong>Chenghao Yang</strong>, Xuezhe Ma., <strong>Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping</strong>, EMNLP 2022 <a
            href="https://arxiv.org/abs/2210.10325">[paper]</a><a href="https://github.com/yangalan123/FineTuningStability"> [codebase]</a></li>
        <li><strong>Chenghao Yang</strong>, Hongyuan Mei, Jason Eisner., <strong>Transformer Embeddings of Irregularly Spaced Events and Their Participants</strong>, ICLR 2022 <a
            href="https://arxiv.org/abs/2201.00044">[full paper]</a><a href="https://github.com/yangalan123/anhp-andtt"> [codebase]</a></li>
        <li>
            <strong>Chenghao Yang</strong>, Yudong Zhang, Smaranda Muresan., 
            <strong>Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains</strong>, ACL 2021 (Short)
            <a href="https://arxiv.org/pdf/2106.02792.pdf">[paper]</a><a href="https://github.com/yangalan123/WM-SRA"> [codebase]</a>
        </li>
        <li>{<strong>Chenghao Yang*</strong>, Yuan Zang*, Fanchao Qi*}, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun.,
            <strong>Word-level Textual Adversarial Attacking as Combinatorial Optimization</strong>, ACL 2020 (Long) <a
                href="https://www.aclweb.org/anthology/2020.acl-main.540.pdf">[paper]</a><a href="https://github.com/thunlp/SememePSO-Attack"> [codebase]</a>
        </li>
        <li>{Fanchao Qi*, Junjie Huang*}, <strong>Chenghao Yang</strong>, Zhiyuan Liu et al., <strong>Modeling Semantic
                Compositionality
                with Sememe Knowledge</strong>, ACL 2019 (Long &amp; Oral) <a
                href="https://arxiv.org/abs/1907.04744">[paper]</a><a href="https://github.com/thunlp/Sememe-SC"> [codebase]</a>
            </li>
    </ol>
    <h3 id="workshop-papers">Workshop Papers</h3>
    <ol>
    <li>Chaoqi Wang, Yibo Jiang, <strong>Chenghao Yang</strong>, Han Liu, Yuxin Chen., <strong>Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints</strong>, SoLAR@NeurIPS 2023, Instruction@NeurIPS 2023 <a href="https://arxiv.org/abs/2309.16240">[paper]</a></li>
        <li><strong>Chenghao Yang*</strong>, Yuhui Zhang*, Zhengping Zhou*, Zhiyuan Liu., <strong>Enhancing Transformer
                with
                Sememe Knowledge</strong>, RepL4NLP@ACL 2020 <a
                href="https://www.aclweb.org/anthology/2020.repl4nlp-1.21/">[paper]</a></li>
        <li>Xiangyang Mou, Mo Yu, Bingsheng Yao, <strong>Chenghao Yang</strong>, Xiaoxiao Guo, Saloni Potdar, Hui Su.,
            Frustratingly Hard Evidence Retrieval for QA Over Books, NUSE@ACL 2020 <a
                href="https://arxiv.org/abs/2007.09878">[paper]</a></li>
    </ol>
    <!-- <h3 id="preprint-work">Preprint</h3>
    <ol>
        
    </ol> -->
    <h2>Service</h2>
    <ul>
    <li><strong>Workshop Organizer:</strong><a href="https://tomworkshop.github.io/">ToM 2023 @ ICML 2023</a></li>
    <li><strong>ARR Reviewer:</strong> {Sept, Oct, Nov} 2021, {March, October} 2022, {December, October} 2023, {February} 2024</li>
    <li><strong>Conference Reviewer:</strong> COLM {2024, 2025}, ICLR {2024, 2025}, NeurIPS {2023, 2025}, EMNLP {2021, 2022}, ACL {2020, 2021}, NAACL 2021, COLING {2020, 2022}, NLPCC 2020</li>
    <li><strong>Workshop Reviewer:</strong>TL4NLP @ NeurIPS 2022</li>
    <li><strong>Social Panel Host:</strong>"Better Developing Pretraining-based Models and Beyond" @ ICLR 2022</li>
    <!-- <li><strong>Secondary Reviewer:</strong> ICLR 2021, ICML 2020, NeurIPS 2019, EMNLP-IJCNLP 2019, ACL 2019</li> -->
    </ul>

    <!-- <a id="misc"><h2>Misc</h2></a>
    <ul>
     <li>I deeply understand how difficult it is to get an NLP Ph.D. offer in US for international students. I have released my <a href="https://drive.google.com/file/d/1n_xcJNVPmhMSrxbQLF9TunXJ9ZM12Mm_/view?usp=sharing">Statement-of-Purpose</a> at <a href="https://cs-sop.org/">cs-sop.org</a>
    <li>In 2021, I have also created a <a href="https://docs.google.com/spreadsheets/d/15Wl3c3Jl-Q1CHd1Zbd7DnpuId-VMZajbF34Wi-rJXTg/edit?usp=sharing">target school list</a> with friends applying for CS Ph.D. program together. It includes deadline and requirements for application materials. Although it is outdated, but usually universities do not change their policies that much and we have included the link to the detailed policy. </li>
     <li>I have mentored, collaborated and advised many great students applying for ML/NLP Ph.D. It turns out they all go to places better than me :-). Proud for them! </li>   
    </li>   
    <li>If you are an undergraduate / master student at UChicago and interested in working with me, please contact my advisors. </li>
    </ul> -->
</div>